{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.datasets import BackboneDataset, CWDataset\n",
    "from models.ResNet50 import ResNet, res50\n",
    "\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "from PIL import ImageFile\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.backends import cudnn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "with open(\"config.yaml\", 'r') as file:\n",
    "    CONFIG = yaml.safe_load(file)\n",
    "\n",
    "# Set the random seeds for reproducibility\n",
    "torch.manual_seed(CONFIG[\"seed\"])\n",
    "torch.cuda.manual_seed_all(CONFIG[\"seed\"])\n",
    "\n",
    "# https://stackoverflow.com/questions/58961768/set-torch-backends-cudnn-benchmark-true-or-not\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cub_path = \"/usr/project/xtmp/cs474_cv/ConceptWhitening/coco_dataset\"\n",
    "train_df = pd.read_parquet(os.path.join(cub_path, CONFIG[\"directories\"][\"data\"], \"train.parquet\"))\n",
    "test_df = pd.read_parquet(os.path.join(cub_path, CONFIG[\"directories\"][\"data\"], \"test.parquet\"))\n",
    "# Ensure reproducibility\n",
    "train_df, val_df = train_test_split(train_df, test_size=len(test_df), random_state=CONFIG[\"seed\"])\n",
    "\n",
    "# Tell PIL not to skip truncated images, just try its best to get the whole thing\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Load the low and high level concept dictionaries\n",
    "low_path = os.path.abspath(CONFIG[\"directories\"][\"low_concepts\"])\n",
    "# high_path = os.path.abspath(CONFIG[\"directories\"][\"high_concepts\"])\n",
    "mappings_path = os.path.abspath(CONFIG[\"directories\"][\"mappings\"])\n",
    "with open(low_path, \"r\") as file:\n",
    "    low_level = json.load(file)\n",
    "\"\"\"with open(high_path, \"r\") as file:\n",
    "    high_level = json.load(file)\"\"\"\n",
    "with open(mappings_path, \"r\") as file:\n",
    "    mappings = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    BackboneDataset(\n",
    "        annotations=train_df,\n",
    "        transform=transforms.Compose([transforms.ToTensor()])\n",
    "    ),\n",
    "    batch_size=CONFIG[\"train\"][\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG[\"train\"][\"workers\"]\n",
    ")\n",
    "\n",
    "# Validation\n",
    "val_loader = DataLoader(\n",
    "    BackboneDataset(\n",
    "        annotations=val_df,\n",
    "        transform=transforms.Compose([transforms.ToTensor()])\n",
    "    ),\n",
    "    batch_size=CONFIG[\"train\"][\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG[\"train\"][\"workers\"]\n",
    ")\n",
    "\n",
    "# Test\n",
    "test_loader = DataLoader(\n",
    "    BackboneDataset(\n",
    "        annotations=test_df,\n",
    "        transform=transforms.Compose([transforms.ToTensor()])\n",
    "    ),\n",
    "    batch_size=CONFIG[\"train\"][\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG[\"train\"][\"workers\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"\"\"train_df_free, free_low_level, free_mappings = CWDataset.make_free_concepts(train_df, 2, low_level,\n",
    "                                                                                high_level, mappings)\"\"\"\n",
    "# TODO: Changed all free_low_level and free_mappings to the defaults\n",
    "high_to_low, low_level_names = CWDataset.generate_low_level_cw_mappings(low_level, mappings)\n",
    "min_concept = min(low_level.values())\n",
    "max_concept = max(low_level.values())\n",
    "\n",
    "concept_loaders = []\n",
    "for i in range(min_concept, max_concept + 1):\n",
    "    concepts = CWDataset(\n",
    "        # train_df_free, low_level, mode=i,\n",
    "        train_df, low_level, mode=i,\n",
    "        transform=transforms.Compose([transforms.ToTensor()])\n",
    "    )\n",
    "\n",
    "    # Sometimes concepts are not in the training set, so we temporarily skip them.\n",
    "    # This can be better addressed in the future.\n",
    "    if len(concepts) == 0:\n",
    "        continue\n",
    "\n",
    "    # Notice that num workers is not set. If there are hundreds of concepts,\n",
    "    # using lots of workers per concept is not practical.\n",
    "    concept_loader = DataLoader(\n",
    "        concepts,\n",
    "        batch_size=CONFIG[\"train\"][\"batch_size\"],\n",
    "        shuffle=True\n",
    "    )\n",
    "    concept_loaders.append(concept_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model = res50(\n",
    "    whitened_layers=CONFIG[\"cw_layer\"][\"whitened_layers\"],\n",
    "    high_to_low=high_to_low,\n",
    "    cw_lambda=CONFIG[\"cw_layer\"][\"cw_lambda\"],\n",
    "    activation_mode=CONFIG[\"cw_layer\"][\"activation_mode\"],\n",
    "    pretrained_model=CONFIG[\"directories\"][\"model\"],\n",
    "    vanilla_pretrain=CONFIG['train']['vanilla']\n",
    ")\n",
    "\n",
    "# Define loss, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG[\"optim\"][\"lr\"],\n",
    "    momentum=CONFIG[\"optim\"][\"momentum\"],\n",
    "    weight_decay=CONFIG[\"optim\"][\"l2\"])\n",
    "scheduler = StepLR(optimizer, step_size=CONFIG[\"optim\"][\"lr_step\"], gamma=CONFIG[\"optim\"][\"lr_gamma\"])\n",
    "\n",
    "model = torch.nn.DataParallel(model, device_ids=list(range(2)))\n",
    "model = model.cuda()\n",
    "\n",
    "# =========================\n",
    "# Print Training Parameters\n",
    "# =========================\n",
    "print(\n",
    "    \"\\nTraining parameters:\\n\"\n",
    "    f\"cw_layer:\\n{textwrap.indent(yaml.dump(CONFIG['cw_layer'], default_flow_style=False), '  ')}\\n\"\n",
    "    f\"train:\\n{textwrap.indent(yaml.dump(CONFIG['train'], default_flow_style=False), '  ')}\\n\"\n",
    "    f\"optim:\\n{textwrap.indent(yaml.dump(CONFIG['optim'], default_flow_style=False), '  ')}\"\n",
    ")\n",
    "\n",
    "# =============\n",
    "# Training Loop\n",
    "# =============\n",
    "if CONFIG[\"verbose\"]:\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "best_acc = 0\n",
    "best_path = None\n",
    "accs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_checkpoint(state):\n",
    "    \"\"\"\n",
    "    Save the model in a compatible format with the ResNet50 `load_model` method.\n",
    "\n",
    "    Params:\n",
    "    - state (dict): Dictionary with keys [epoch, acc, state_dict]. Contains the model's state information.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the saved model file.\n",
    "    \"\"\"\n",
    "    path = os.path.join(CONFIG[\"directories\"][\"checkpoint\"],\n",
    "                        f\"{CONFIG['train']['checkpoint_prefix']}_epoch_{state['epoch']}_acc_{state['acc']:.4f}.pth\")\n",
    "    torch.save(state[\"state_dict\"], path)\n",
    "    return path\n",
    "\n",
    "\n",
    "def top_k_correct(output, target: torch.Tensor, k=1):\n",
    "    \"\"\"\n",
    "    See how many predictions were in the top k predicted classes. Assumes target is already adjusted to be -1 for CUB.\n",
    "    \"\"\"\n",
    "    _, predicted_topk = torch.topk(output, k, dim=1)\n",
    "    correct_topk = (predicted_topk == target.unsqueeze(1)).sum().item()\n",
    "    return correct_topk\n",
    "\n",
    "\n",
    "def top_k_activated_concepts(concept_loaders, data_loader: DataLoader[BackboneDataset], model: nn.DataParallel[ResNet],\n",
    "                             output_path: str, low_level_names: dict[int, str], k=50):\n",
    "    \"\"\"\n",
    "    This should only be run at the end of the training cycle, as it sets the model to evaluation mode.\n",
    "    The activations in the last CW layers will be considered.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Update the gradient matrix G for the concept whitening layers.\n",
    "        # Each concept in the CWLayer is indexed by its corresponding position in concept_loaders.\n",
    "        for idx, concept_loader in enumerate(concept_loaders):\n",
    "            # if idx not in CONFIG['train']['allowed_concepts']:\n",
    "            #    continue\n",
    "            model.module.change_mode(idx)\n",
    "\n",
    "            for batch, region in concept_loader:\n",
    "                batch: torch.Tensor\n",
    "                batch = batch.cuda()\n",
    "                model(batch, region, batch.shape[2])  # batch.shape[2] gives the original x dimension\n",
    "                break  # only sample one batch for each concept\n",
    "\n",
    "        model.module.reset_counters()\n",
    "        # Stop computing the gradient for concept whitening.\n",
    "        # mode=-1 is the default mode that skips gradient computation.\n",
    "        model.module.change_mode(-1)\n",
    "\n",
    "    idx = -1\n",
    "    last_cw_layer = model.module.cw_layers[idx]\n",
    "\n",
    "    hook = last_cw_layer.register_forward_hook(ResNet.get_X_activated)\n",
    "    output = {}\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for input, path, target in data_loader:\n",
    "            input: torch.Tensor\n",
    "            target: torch.Tensor\n",
    "\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "            # List of torch.Tensors of size batch_size x k, each corresponding to a relevant CW layer\n",
    "            batch_top_k_vals, batch_top_k_concepts = model.module.top_k_activated_concepts(input, k, idx)\n",
    "            batch_top_k_vals = batch_top_k_vals.cpu()\n",
    "            batch_top_k_concepts = batch_top_k_concepts.cpu()\n",
    "\n",
    "            # Loops through each image in the batch\n",
    "            for image_index in range(input.shape[0]):\n",
    "                image_path = path[image_index]\n",
    "                top_k_concepts = batch_top_k_concepts[image_index].tolist()\n",
    "                top_k_vals = batch_top_k_vals[image_index].tolist()\n",
    "\n",
    "                # Translates from concept index to concept name\n",
    "                top_k_names = list(low_level_names[concept] for concept in top_k_concepts)\n",
    "\n",
    "                output[image_path] = [top_k_vals, top_k_names]\n",
    "\n",
    "    hook.remove()\n",
    "\n",
    "    with open(output_path, 'w') as json_file:\n",
    "        json.dump(output, json_file, indent=4)\n",
    "    print(f\"Top k activated concepts outputs have been saved to '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_loader: DataLoader,\n",
    "    concept_loaders: list[DataLoader],\n",
    "    model: nn.DataParallel[ResNet],\n",
    "    criterion: nn.modules.loss._Loss,\n",
    "    optimizer: torch.optim.Optimizer\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    model.train()\n",
    "\n",
    "    for i, (inp, _, target) in enumerate(train_loader):\n",
    "        inp: torch.Tensor\n",
    "        target: torch.Tensor\n",
    "        print(inp.shape)\n",
    "        print(target.shape)\n",
    "        # NOTE: CUB dataset labels start at 1, hence this line. If your target starts at zero, this needs to be removed!\n",
    "        #target = target - 1\n",
    "\n",
    "        # Train for concept whitening loss once every train_cw_freq batches.\n",
    "        if False and (i + 1) % CONFIG[\"train\"][\"train_cw_freq\"] == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Update the gradient matrix G for the concept whitening layers.\n",
    "                # Each concept in the CWLayer is indexed by its corresponding position in concept_loaders.\n",
    "                for idx, concept_loader in enumerate(concept_loaders):\n",
    "                    # if idx not in CONFIG['train']['allowed_concepts']:\n",
    "                    #    continue\n",
    "                    model.module.change_mode(idx)\n",
    "\n",
    "                    for batch, region in concept_loader:\n",
    "                        batch: torch.Tensor\n",
    "                        batch = batch.cuda()\n",
    "                        model(batch, region, batch.shape[2])  # batch.shape[2] gives the original x dimension\n",
    "                        break  # only sample one batch for each concept\n",
    "\n",
    "                model.module.update_rotation_matrix()\n",
    "                # Stop computing the gradient for concept whitening.\n",
    "                # A mode of -1 is the default mode that skips gradient computation.\n",
    "                model.module.change_mode(-1)\n",
    "            model.train()\n",
    "\n",
    "        # Move them to CUDA, assumes CUDA access\n",
    "        print(f'shape: {inp.shape} \\n item:{inp[0]}')\n",
    "        inp = inp.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        # Forward pass + loss computation\n",
    "        output = model(inp)\n",
    "        loss: torch.Tensor = criterion(output, target)\n",
    "\n",
    "        # Performance metrics\n",
    "        total_loss += loss.item()\n",
    "        total_correct += top_k_correct(output, target)\n",
    "\n",
    "        # Calculate CW loss once every train_cw_freq batches\n",
    "        if (i + 1) % CONFIG[\"train\"][\"train_cw_freq\"] == 0:\n",
    "            model.module.reset_counters()\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Each concept in the CWLayer is indexed by its corresponding position in concept_loaders.\n",
    "                for idx, concept_loader in enumerate(concept_loaders):\n",
    "                    # if idx not in CONFIG['train']['allowed_concepts']:\n",
    "                    #     continue\n",
    "                    model.module.change_mode(idx)\n",
    "\n",
    "                    for batch, region in concept_loader:\n",
    "                        batch: torch.Tensor\n",
    "                        batch = batch.cuda()\n",
    "                        model(batch, region, batch.shape[2])  # batch.shape[2] gives the original x dimension\n",
    "\n",
    "                # Stop computing the gradient for concept whitening.\n",
    "                # A mode of -1 is the default mode that skips gradient computation.\n",
    "                model.module.change_mode(-1)\n",
    "            model.train()\n",
    "\n",
    "            # This is really CW score not loss. We want to maximize axis alignment. Hence the minus\n",
    "            print(f\"Main objective loss: {loss}\", flush=True)\n",
    "\n",
    "            cw_loss = model.module.cw_loss()\n",
    "            print(f\"CW score: {cw_loss}\", flush=True)\n",
    "\n",
    "            loss -= CONFIG[\"train\"][\"cw_loss_weight\"] * cw_loss\n",
    "            model.module.reset_counters()\n",
    "\n",
    "        # Compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    end = time.time()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    acc = total_correct / len(train_loader.dataset)\n",
    "\n",
    "    return avg_loss, acc, end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    # Train and validate an epoch\n",
    "    _, train_acc, train_duration = train(train_loader, concept_loaders, model, criterion, optimizer)\n",
    "    accs.append(validate(val_loader, model, criterion)[1])\n",
    "\n",
    "    # Learning rate scheduler step. Every 30 epochs, learning rate lr is divided by 5.\n",
    "    scheduler.step()\n",
    "\n",
    "    # Compute average accuracy of last 5 epochs\n",
    "    num_epochs = len(accs)\n",
    "    last_n_accuracies = accs[-min(num_epochs, 5):]\n",
    "    avg_acc = sum(last_n_accuracies) / len(last_n_accuracies)\n",
    "\n",
    "    # Only save if the average accuracy is better\n",
    "    is_best = (avg_acc > best_acc)\n",
    "    if is_best:\n",
    "        best_acc = avg_acc\n",
    "        # We'll need to reload the best model, so save the path to its checkpoint\n",
    "        if best_path:\n",
    "            os.remove(best_path)  # If this is not the first epoch, overwrite the previous path\n",
    "        best_path = save_checkpoint({\"epoch\": epoch + 1, \"state_dict\": model.state_dict(), \"acc\": avg_acc})\n",
    "\n",
    "    if (CONFIG[\"verbose\"]) and ((epoch + 1) % CONFIG[\"train\"][\"print_freq\"] == 0):\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1} complete\\n\"\n",
    "            f\"\\tDuration: {train_duration:.2f}s\\n\"\n",
    "            f\"\\tTrain Accuracy: {train_acc:.4f}\\n\"\n",
    "            f\"\\tValidation Accuracy: {accs[-1]:.4f}, was best? {is_best}\",\n",
    "            flush=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_path:\n",
    "    model.module.load_model(best_path)\n",
    "\n",
    "_, val_acc = validate(test_loader, model, criterion)\n",
    "if CONFIG[\"verbose\"]:\n",
    "    print(f\"Training completed. Final Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Obtain the top k activated concepts for each image\n",
    "if CONFIG[\"eval\"][\"top_k_concepts\"]:\n",
    "    output_path = os.path.join(CONFIG[\"directories\"][\"eval\"],\n",
    "                                f\"top_k_concepts_{CONFIG['train']['checkpoint_prefix']}.json\")\n",
    "    top_k_activated_concepts(concept_loaders, test_loader, model, output_path, low_level_names,\n",
    "                                CONFIG[\"eval\"][\"k_concepts\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
